%%% CBC SEARCH %%%
% Search Methods
%   Gravitational Wave Data
%   Matched Filter
%   Phase Maximisation
%   Template Bank
%   Ranking Statistic
%       Coincident Detection
%       Signal-consistency tests
%       Template Fits
%       PSD Variation
% PyCBC Live Configuration
%   Low-Latency Detection
%   SNR Optimization
%   Ranking Statistic
%
Search pipelines, such as PyCBC Live, are used to analyse data from \gw detectors to detect \gw signals both in real-time (live) and afterwards (offline). In this chapter we introduce the techniques used by the PyCBC search pipelines to detect astrophysical signals buried in \gw data.

\section{\label{}Gravitational Wave Data}

\Gw data refers to that which is received from the gravitational wave detectors and is a time-series recording of the dimensionless strain measured by the detector for a stretch of time. The data, $s(t)$, is formed of a noise component, $n(t)$, and an astrophysical signal, $h(t)$, such that
%
\begin{equation}
    s(t) =
    \begin{cases}
        n(t), & \text{no signal} \\
        n(t) + h(t), & \text{signal}
    \end{cases}
\end{equation}

\section{\label{}Search Methods}
\Gws are difficult to find in \gw data due to the continuous parameter space and potentially infinite number of possible combinations of \gw signal parameters. In this section we will begin by briefly discussing how we model \gw waveforms in section~\ref{sec:waveform-modelling}, then we move onto signal processing techniques using these waveforms and the data in sections~\ref{sec:matched-filter}--\ref{sec:template-bank}.

\subsection{\label{sec:waveform-modelling}Waveform Modelling}

% Very brief
% Number of parameters
% Description of parameters

The waveform is composed using 15 parameters, split into intrinsic and extrinsic parameters. Intrinsic parameters describe physical properties of the system whereas extrinsic parameters describe things related to the detector network, position and/or orientation on the sky \cite{BAYEstar}.

Intrinsic parameters:
\begin{itemize}
   \item $m_1$, primary mass
   \item $m_2$, secondary mass
   \item $\vec{\chi_1}$, primary spin
   \item $\vec{\chi_2}$, secondary spin
\end{itemize}

Extrinsic parameters:
\begin{itemize}
   \item $\alpha$, right ascension
   \item $\delta$, declination
   \item $r$, distance
   \item $t_c$, time of coalescence
   \item $\iota$, inclination angle
   \item $\psi$, polarization angle
   \item $\phi_c$, coalescence phase
\end{itemize}

We create our waveform template using these parameters however, a bank of waveform templates in 15 dimensions will require an obscene number of templates to sufficiently cover. We can reduce the dimensionality by considering only signals which have no orbital precession or eccentricity. 

\subsection{\label{sec:matched-filter}Matched Filter}

Using GR, we can produce waveforms which represent a \gw in our photodetector output. Knowing what we are looking for allows us to use matched filtering, a signal processing technique where a known signal is correlated with data to
obtain a signal-to-noise ratio (SNR) informing us of the likelihood of the waveform being in the data

To being we can the data as a sum of a signal and noise:

\begin{equation}
   s(t) = h(t) + n(t)
   \label{eqn:s_h_n}
\end{equation}

Where $s$ is the data, $h$ is a signal (in this case, a real \gw signal) and $n$ is the noise. Using a template of the signal, we can use matched filtering to find the signal within our data.

The matched filter is defined as \cite{PyCBC_Search}:

\begin{equation}
   \rho^2(t) = \frac{(s|h)^2}{(h|h)}
   \label{eqn:matched_filter}
\end{equation}

The weighted inner product of the data, $s$, and the signal, $h$, in the frequency domain producing a signal-to-noise ratio (SNR), $\rho^2(t)$, in the time domain. We define the inner product:

\begin{equation}
   (s|h)(t) = 4 \Re \int^{+\infty}_{-\infty} \frac{\tilde{s}(f) \tilde{h}^*(f)}{S_n(f)} e^{2 \pi i f t} df
   \label{eqn:inner_product}
\end{equation}

A tilde, e.g. $\tilde{s}(f)$, denotes a Fourier transformed version of the variable.

\begin{equation}
   \tilde{s}(f) = \int^{+\infty}_{-\infty} s(t) e^{-2 \pi i f t} dt
   \label{eqn:fourier_transform}
\end{equation}

In equation \ref{eqn:inner_product} we have Fourier transformed data, $\tilde{s}(f)$, and a Fourier transformed signal, $\tilde{h}(f)$.

On the denominator within the integral in equation \ref{eqn:inner_product} $S_n(f)$ is the power spectral density (PSD) of the detector noise:

\begin{equation}
   \langle \tilde{n}(f) \tilde{n}(f') \rangle = \frac{1}{2} S_n(f) \delta(f-f')
   \label{eqn:PSD}
\end{equation}

The angle brackets denote averaging over noise realizations, $n$ is stationary Gaussian detector noise and $\delta$ is the Dirac delta function.

These equations build a tool that allows us to analyse a data to look for a signal. One requirement is that we must know the signal we are looking for before we search for it. We can create a template of our signal, referred to as a waveform. We then matched filter our data with the waveform and search through the resulting SNR timeseries for a peak, indicating the presence of the signal.

\subsection{\label{sec:phase-maximisation}Phase Maximisation}

Furthermore, the matched filter maximises the amplitude of the \gw signal and we have the ability to maximise over the phase of the signal by using an altered version of the matched filter:

\begin{equation}
   \text{$\rho^2_{\Phi,max}(t) = \frac{(s|h)^2 + (s|h_{\pi/2})^2}{(h|h)}$ where $\tilde{h}_{\pi/2}(f) = i \tilde{h}(f)$}
   \label{eqn:phase_mf}
\end{equation}

Where $\tilde{h(f)}$ and $\tilde{h}_{\pi/2}(f)$ are orthogonal phases of our template.

\subsection{\label{sec:template-bank}Template Bank}

Our waveform template bank will be only 5 parameters: the two masses, the two free spins and, the time of coalescence. We must create a bank of waveform templates describing all the possible signals and template density of the bank needs to be fine-tuned in order to ensure two things: our template bank covers the parameter space well enough to find all signals and our template bank isn't so large we are wasting computational time and resources \cite{Owen:tbank}.

\section{\label{sec:ranking-statistic}Ranking Statistic}
The ranking statistic is a measure of confidence that a signal originated from an astrophysical source as opposed to noise in the detectors. We use the ranking statistic to calculate a false alarm rate which informs us of the number of events per unit time that will originate from noise only. There must be a balance struck when choosing a false alarm rate at which events are determined to be real, require a very small false alarm rate and you will potentially exclude real events from a catalogue and allow too high of a false alarm rate and you will potentially include noise in the catalogue. In this section we will describe the different components which make up the ranking statistic.

\subsection{\label{sec:coincident-detection}Coincident Detection}

We want to see the same signal in more than one detector.

\subsection{\label{sec:signal-consistency-tests}Signal Consistency Tests}

We want the signal to follow the expected power distribution for that signal.

\subsection{\label{sec:template-fitting}Template Fitting}

When a template triggers with a large, we check to see if it commonly triggers with a large snr on noise historically. If it does then we can downweight our confidence that it is legitimate. If the template rarely ever triggers with a high snr on noise then we can increase our confidence in the detection.

We fit the distribution of triggers and snr to an exponential and the coefficient of the exponential is the number we use to weight.

How the fits are created:
- binning
- smoothing

\subsection{\label{sec:psd-variation}PSD Variation}

If the detector is behaving terribly we can model that and include it in the ranking statistic.

\subsection{\label{sec:kde-statistic} KDE Statistic}

\section{\label{sec:pycbc-live}PyCBC}

% Overarching points:
% - State of the field before my work
% - Overview of PyCBC Live in O3
% - Avoid talking about new developments

% What to talk about in this section:
% - Offline vs Live differences
% - Difference in computation of ranking statistic
% - How is ranking computed in Live
% -     Focus on methodology instead of code
% - O3 it wasn't able to cmopute some of the quantities that offline could
% -     Particularly things based on bulk statistics
% - Results in PyCBC Live being less sensitive
% -     List number of offline detections vs num live detections
% -     R&P paper demonstrated PyCBC Live is the most sensitive pipeline in all regions of parameter space in O3
% -     But gstlal had more events than pycbc (cross compare numbers and look at other pipelines)

% --------------------------------------------------

% Introduction to the section

This section combines the previously discussed techniques into an end-to-end pipeline used to detect gravitational waves. We will discuss primarily the PyCBC Live real time search for gravitational waves and how this search differs from the PyCBC offline search for gravitational waves including the limitations and the methodology differences to overcome these limitations.

\section{\label{sec:offline-vs-live}Offline vs Live}
% Differences in offline vs live

The search for gravitational waves occurs over two different time-scales. There is a "real-time" live search and an offline "post-detection" search, these searches have broadly the same goals--to detect gravitational waves--however they have different limitations, especially in the case of the live searches. Where the offline search has more time and potentially computing power available to probe the data more deeply, the live search is capable of rapidly sending information of potentially electromagnetically bright events to other astronomical observatories to perform multi-messenger astronomy.

PyCBC runs three searches currently for the fourth observing run, two live searches and an offline search. The live searches are the full-bandwidth complete template bank search which uses a template bank of \~730k templates to search for all known gravitational wave signals and, an early warning search which uses a much smaller \~9k template bank comprised of only electromagnetically bright frequency truncated signals intended to observe gravitational wave signals before the merger occurs. The third search currently running is the offline search which also uses a large bank of \~696k templates and want to observe all gravitational wave signals in the data.

This section will describe the PyCBC Live search in greater detail, focussing on the optimizations and unique components that differ it from the offline search and how these help to improve our detection of gravitational waves. The full bandwidth search is being described in these sections, the early warning search varies every so slightly but not enough to require distinctions to be made.

\subsection{\label{sec:low-latency-detection}Low-Latency Detection}

The PyCBC Live search aims to detect all gravitational waves in real time. This is a feat of software engineering to obtain the gravitational wave data, perform the matched filtering of hundreds of thousands of templates, perform signal consistency tests and output an approximate sky map within tens of seconds of the merger of the gravitational wave passing through the detectors.

The live search is distributed over 151 computing nodes and these are all connected through a message passing interface which tracks the individual search nodes. The main ranked node will receive messages from each of these nodes and distribute information and jobs to all the nodes. For example, when a node running a matched filter on H1 data finds a trigger and a node running a matched filter on L1 data finds a trigger, these triggers are sent back to the rank 0 node which then assigns another node to perform the coincidence tests as part of the ranking statistic to determine if this trigger was a real gravitational wave event.

The template bank of 730 thousand template distributed throughout the nodes and pre-generated in memory so the matched filters do not have to rely on generating the templates every time new data arrives. The template bank is shuffled into a random order so that the distribution of `long' and `short' templates is randomly distributed throughout the nodes. If a single node has all the long templates it will take longer to perform the matched filters than the nodes with the short templates and this can introduce lagging effects if the search is being held up by a single node.

The live search holds a continuous buffer of data where the newest eight seconds of data are added to the front and push the oldest eight seconds of data out of the buffer. The data is then matched filtered with the template bank and each node reports on the triggers that have been found. The hope with the search is that all of this processing can happen before the next eight seconds of data arrives at the detector. (PLOT: PYCBC LIVE LAG PLOT DIAGRAM THING)

\subsection{\label{sec:snr-optimization}SNR Optimization}

We upload potential detections of gravitational wave events to a central public database called GraceDB. These events are then used by astronomers to observe potential electromagnetic counterparts. We want to report the best information we can and produce the most accurate sky map. Increasing the SNR of the event is an easy way to improve this, having a better matching template to the signal will do this. (REWRITE).

The SNR optimization module of PyCBC Live takes a gravitational wave event and uses optimization functions to rapidly optimize the gravitational wave parameters to retrieve small fractions of improvement to the SNR. The original SNR of the event is limited by the template bank placement and the SNR optimizer allows a much finer exploration of the local parameter space to the best found template in the template bank. (ADD PLOT SHOWING TEMPLATE BANK WITH TEMPLATES NEARBY THE ACTUAL PARAMETER VALUES AND SNR IMPROVEMENT).

\subsection{\label{sec:live-ranking-statistic}Ranking Statistic}

The PyCBC Live ranking statistic run in the third observing run and the first half of the fourth observing run was very simple, only taking into account the Allen chisq, sine-gaussian chisq and phasetd. These are basic signal consistency tests for single detector checks, re-ranking the snr depending on how the morphology of the snr evolves over the signal. In coincidence the ranking statistic only looks for phase and template consistency for the two triggers and a time coincidence so that both detector triggers fall in light time travel time.



% Removed from pycbclive.tex
This chapter details the changes made to the PyCBC Live ranking statistic to improve on the original ranking statistic used in the third and fourth observing runs and detailed in~\ref{sec:live-ranking-statistic}. The two additions that were made to the ranking statistic were the inclusion of PSD variation, to track the non-stationary of the detector noise, and adding template fits to the statistic to weigh triggers based on their previously measured SNR distribution.

In this chapter we introduce the PyCBC Live search for CBC signals and the improvements made to the PyCBC Live search's ranking statistic to improve the sensitivity of the search and the confidence of our gravitational wave detections. The PyCBC Live search has been operational since the 2018~\cite{PyCBC_live} and has been in constant operation since, contributing to the 2nd, 3rd and 4th observing runs and detecting over 200 gravitational wave events. The PyCBC Live search processes gravitational wave detector data using techniques described in section~\ref{sec:Search} however, there are a number of optimizations and new techniques used to enable to low-latency rapid detection of gravitational wave events in close to real time.

PyCBC Live uses a ranking statistic post-detection of a gravitational wave event candidate which provides a numerical confidence in the event being real. This number is referred to as the `False Alarm rate' (FAR) and is measured in units of events per unit time (typically per year or Hertz). The inverse of the false alarm rate (IFAR) is commonly used and is typically measured in units of years. The false alarm rate tells us how often a candidate event would be observed due to random noise in the absence of a real astrophysical signal. IFAR can be thought of as the expected time interval between random noise events that could produce a signal resembling the observed candidate event. A higher IFAR indicates a longer expected waiting time between false alarms.

The LVK places a limit on the IFAR of a signal before it can be determined to be real. The limit has to be placed to balance the sensitivity of a search and the purity of a gravitational wave catalogue. As a quick example, if the IFAR limit is set to 1 year. Then one might expect to see one event per year in the catalog purely due to noise.

We begin this chapter by detailing the components that make up the ranking statistic and then the new components of the improved ranking statistic and how these are derived and used.

The layout of this chapter is as follows. In section~\ref{sec:live_optimizations} we detail the key components of t

